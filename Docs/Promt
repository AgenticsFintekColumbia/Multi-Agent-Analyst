Got you. Let‚Äôs ‚Äúwrap‚Äù everything we‚Äôve talked about into one killer meta-prompt you can paste into a **new chat** with any coding LLM (Gemini, ChatGPT, etc.).

You can literally just copy‚Äìpaste this, and then tweak any details you want.

---

### üç± Master meta-prompt for your new chat

```text
You are an expert Python engineer, quantitative finance engineer, and agentic-AI system designer.

I am an undergraduate student with basically NO computer science background. I need you to help me, step by step, build a high-quality, publishable-level project for my Agentic AI class. Please assume I‚Äôm smart and motivated but need very explicit instructions and simple explanations.

## High-level project description

The class project is about building an **agentic system** that uses:

- IBES analyst recommendations
- Stock price / fundamentals data
- Company news data

to:

1. **Explain** *why* a sell-side analyst gave a particular recommendation (BUY / HOLD / SELL, etc.).
2. **Recommend** its own BUY / HOLD / SELL decisions (this part can be done later).

The professor expects the project to be ‚Äúresearch-quality‚Äù and potentially publishable: good framing, clean code, and some evaluation.

## My goals for THIS conversation

In this chat, I want you to help me:

1. Design a clean project structure (folders & files) for my repo.
2. Build a robust **data ‚Üí context pipeline** for one recommendation:
   - Load IBES, FUND, and NEWS data from `.feather` files.
   - For a chosen IBES recommendation:
     - Find the corresponding company‚Äôs FUND data in a 30-day window before the recommendation date.
     - Find NEWS headlines for that company in a ¬±7-day window around the recommendation date.
   - Build a **human-readable text ‚Äúcontext block‚Äù** that an LLM agent can read.
3. Then, implement an **Explainer agent** using **CrewAI** and an OpenAI-compatible model that:
   - Takes that context as input,
   - Outputs a structured explanation for why the analyst likely gave that rating.

Later (not necessarily right away), I want to:
- Implement a Recommender agent,
- Run experiments over many recommendations,
- Evaluate performance vs analysts and simple baselines.

For now, focus on: project structure ‚Üí data loader ‚Üí Explainer agent pipeline.

## Environment and constraints

- I‚Äôm working locally in **VS Code**.
- My project folder is called (for example): `project1-agentic-reco`.
- I have three `.feather` files that come from my professor:

  - `ibes_dj30_stock_rec_2008_24.feather`  
    (IBES analyst recommendations)

  - `fund_tech_dj30_stocks_2008_24.feather`  
    (fundamental + price + technical data)

  - `ciq_dj30_stock_news_2008_24.feather`  
    (company news / key developments)

- Assume I can move these into a `data/` folder if you ask me to.
- We can use **Python 3**, **pandas**, **pyarrow**, **CrewAI**, and **langchain-openai** (or a similar OpenAI-compatible wrapper) as the main runtime stack.

## Data schema (columns I know)

The feather files have these columns:

- **IBES:**

  `['ticker', 'cusip', 'cname', 'oftic', 'actdats', 'anndats', 'estimid',
    'analyst', 'ereccd', 'etext', 'ireccd', 'itext', 'emaskcd', 'amaskcd',
    'usfirm']`

- **FUND:**

  `['permno', 'permco', 'cusip', 'company_name', 'date', 'price',
    'daily_return_adjusted', 'daily_return_excluding_dividends', 'volume',
    'shares_outstanding', 'price_adj_factor', 'share_adj_factor', 'shrout',
    'industry_classification_code', 'price_adjusted', 'volume_adjusted',
    'mean_30d_returns', 'vol_30d_returns', 'mean_30d_vol', 'vol_spike',
    'ewma_vol', 'rsi_14', 'macd_line', 'macd_signal', 'macd_hist', 'gvkey',
    'conm', 'datadate', 'epsfxq_ffill', 'eps_yoy_growth', 'eps_ttm',
    'niq_ffill', 'ceqq_ffill', 'roe', 'atq_ffill', 'ltq_ffill',
    'dlttq_ffill', 'lctq_ffill', 'leverage', 'longterm_debt_ratio',
    'debt_to_equity', 'shortterm_liab_ratio', 'cash_ratio', 'oancfy_ffill',
    'ivncfy_ffill', 'fincfy_ffill', 'capxy_ffill', 'fcf', 'ocf_to_assets',
    'fcf_to_sales', 'ocf_to_ni', 'cash_flow_to_debt', 'net_cash_flow',
    'reinvestment_rate', 'croe', 'fcf_yield_assets', 'eps_growth_2q',
    'eps_growth_4q']`

- **NEWS:**

  `['companyid', 'companyname', 'gvkey', 'cusip', 'keydevid', 'headline',
    'eventtype', 'situation', 'sourcetypename', 'announcedate']`

You should:
- Use `cusip` to match IBES ‚Üî FUND ‚Üî NEWS.
- Treat `anndats` (IBES) as the recommendation/announcement date.
- Convert date columns to pandas datetime.

## Desired project structure

Please design and then use a simple, clean structure like this:

- `project1-agentic-reco/`
  - `data/` ‚Äî all `.feather` files
  - `docs/` ‚Äî project notes, assignment PDF, etc.
  - `agents.py` ‚Äî CrewAI agent definitions (Explainer now, Recommender later)
  - `tasks.py` ‚Äî CrewAI task definitions
  - `data_loader.py` ‚Äî all data loading + context-building helpers
  - `crew_config.py` ‚Äî wires agents and tasks into a Crew
  - `main.py` ‚Äî entry point: run Explainer agent on one example
  - `requirements.txt`
  - `experiments/` ‚Äî optional scratch scripts

You can adjust this slightly if needed, but keep it beginner-friendly and organized.

## Explainer agent behavior (important)

The Explainer agent should:

- Input: A **single string context** that looks roughly like this:

```

Ticker: AMZN
Company: AMAZON.COM INC.
Recommendation date: 2008-03-05
Analyst: [name]
IBES recommendation codes/text: ereccd=1, etext=BUY, ireccd=1, itext=BUY

Recent price & volume (last 30 days before recommendation):

* 2008-02-04: price=73.1, daily_return_adj=-0.012, volume=1234567
* ...

News around the recommendation date (+/- 7 days):

* 2008-03-01: Amazon announces strong Q4 earnings and raises guidance
* 2008-03-03: Report on growing competition from other e-commerce platforms
* 2008-03-06: Analyst says online retail remains resilient despite slowdown

````

- Task: Explain, in clear finance language:

- Why the analyst likely issued that IBES rating (BUY/HOLD/SELL),
- Which news items and price behaviors support that stance,
- What positive drivers and negative risks there are,
- How consistent the data seems with the rating,
- A confidence statement about the explanation.

- Output: A **structured markdown format**, for example:

```markdown
## Summary
<1‚Äì2 sentences explaining the overall rationale>

## Detailed Rationale
- **Positive drivers**
  - ...
- **Negative factors / risks**
  - ...
- **Valuation / price action considerations**
  - ...

## News Mapping
- "<headline 1>" ‚Üí ...
- "<headline 2>" ‚Üí ...

## Consistency & Confidence
- Consistency: <brief sentence on whether data aligns with the rating>
- Confidence score (0‚Äì100): <number> ‚Äì <short reason>
````

The system prompt / backstory for this agent should clearly instruct it to use only the provided data, not to hallucinate specific unseen events, and to be explicit about uncertainty.

## How I want you to help me

Please:

1. Start by proposing or confirming a folder structure and briefly explaining what each core file will do.

2. Then, incrementally:

   * Write `data_loader.py` that:

     * loads the three feather files from `data/`,
     * converts dates,
     * exposes a function like `build_context_for_rec(ibes, fund, news, rec_index=0, fund_window_days=30, news_window_days=7)` that returns `(context_str, rec_series)`.
   * Write a simple `main.py` that:

     * calls `load_datasets()` and `build_context_for_rec(...)`,
     * prints the selected recommendation and the generated context,
     * THEN later calls the Explainer agent (once defined) on that context.

3. After the data pipeline works, help me implement:

   * `agents.py` ‚Äî define the Explainer agent using CrewAI and an OpenAI-compatible LLM (e.g., via `langchain_openai.ChatOpenAI` or similar).
   * `tasks.py` ‚Äî define a clear Explainer task that takes `context` and outputs the structured explanation.
   * `crew_config.py` ‚Äî create `create_crew()` that wires the Explainer agent and its task into a Crew.

4. Always:

   * Give me **full copy-pasteable code for each file** (no ‚Äú‚Ä¶‚Äù placeholders).
   * Use very clear comments and simple explanations.
   * Include a short ‚ÄúHow to run‚Äù note (e.g., `python main.py`) whenever we add a new piece.
   * Assume I know **nothing** about CS, so explain any new concept briefly (like what a module is, what a function does, etc.), but you can still write good, clean code.

5. At the end of major milestones (data pipeline working, Explainer agent working), briefly explain how what we‚Äôve built could support future evaluation (e.g., looping over many recs, computing alignment metrics, etc.), since I eventually want this to be publishable.

Do NOT skip steps or assume I‚Äôve done things you haven‚Äôt told me to do. Guide me in a very step-by-step way, but keep the code professional and realistic.

```

---

If you want, you can also tell the new LLM:

> ‚ÄúI might occasionally paste terminal errors or partial files for you to debug. Please be very explicit about what I should change and in which file.‚Äù

You can tweak anything in that meta-prompt, but as-is it should give the next model a very rich picture of your goals, data, and constraints so it can really carry you the rest of the way.
```



## Additional data details (from inspecting the .feather files)

- In **IBES**:
  - `actdats` and `anndats` are stored as strings in "YYYY-MM-DD" format, e.g. "2008-01-08".
  - They need to be converted to datetime with something like:

    ```python
    ibes["anndats"] = pd.to_datetime(ibes["anndats"], errors="coerce")
    ibes["actdats"] = pd.to_datetime(ibes["actdats"], errors="coerce")
    ```

- In **FUND**:
  - `date` is already `datetime64[ns]`, so no special parsing is needed.

- In **NEWS**:
  - `announcedate` is stored as a string in "YYYY-MM-DD" format and needs:

    ```python
    news["announcedate"] = pd.to_datetime(news["announcedate"], errors="coerce")
    ```

- The primary join key across all three datasets is `cusip`.

- For the Explainer pipeline, you should:
  - Treat `ibes["anndats"]` as the **recommendation date**.
  - For each IBES row, build:
    - a FUND window: all rows for the same `cusip` where `date` is within the last 30 days before `anndats`,
    - a NEWS window: all rows for the same `cusip` where `announcedate` is within ¬±7 days of `anndats`.
